{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596925d8-6132-4f90-8bdb-a9dff75e14f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e372d7-cd24-4ecb-88a3-0a979c6aec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/a100_nas_lvbo/peixunban/002754_lvbo/OLMo/scripts/convert_olmo_to_hf_new.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded = torch.load(os.path.join(input_base_path, \"model.pt\"), map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all parameters from the checkpoint at /root/a100_nas_lvbo/peixunban/002754_lvbo/save_check/olmo1b_2024_12_09_01_44_49/latest-unsharded.\n",
      "Saving a GPTNeoXTokenizerFast to /root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/hf_pt/olmo1b_2024_12_09_01_44_49_6000.\n",
      "Loading the checkpoint in a OLMo model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:04<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving in the Transformers format.\n",
      "[2024-12-11 09:55:33,002] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/a100_nas_lvbo/peixunban/002754_lvbo/OLMo/scripts/convert_olmo_to_hf_new.py\", line 283, in <module>\n",
      "    main()\n",
      "  File \"/root/a100_nas_lvbo/peixunban/002754_lvbo/OLMo/scripts/convert_olmo_to_hf_new.py\", line 271, in main\n",
      "    write_model(\n",
      "  File \"/root/a100_nas_lvbo/peixunban/002754_lvbo/OLMo/scripts/convert_olmo_to_hf_new.py\", line 195, in write_model\n",
      "    shutil.rmtree(tmp_model_path)\n",
      "  File \"/root/miniconda3/envs/llama-factory/lib/python3.10/shutil.py\", line 731, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/root/miniconda3/envs/llama-factory/lib/python3.10/shutil.py\", line 729, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/hf_pt/olmo1b_2024_12_09_01_44_49_6000/tmp'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python /root/a100_nas_lvbo/peixunban/002754_lvbo/OLMo/scripts/convert_olmo_to_hf_new.py --input_dir /root/a100_nas_lvbo/peixunban/002754_lvbo/save_check/olmo1b_2024_12_09_01_44_49/latest-unsharded  \\\n",
    "        --tokenizer_json_path /root/a100_nas_lvbo/peixunban/002754_lvbo/OLMo/olmo_data/tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json \\\n",
    "        --output_dir /root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/hf_pt/olmo1b_2024_12_09_01_44_49_6000 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e92992b-465f-41eb-a9f5-f3eb8fabd14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an intelligent assistant capable of providing accurate and well-structured answers. Please answer the following question in detail. Question: Explain the significance of renewable energy sources in combating climate change. Provide examples of commonly used renewable energy sources, their advantages, and any challenges associated with their adoption. Make your response structured into three parts: 1) Introduction, 2) Key Examples with Advantages, and 3) Challenges and Conclusion. For a quick overview, refer to the relevant chapters on Renewable Energy Sources in Regenerative Medicine, or to other topics on Renewable Energy Sources from:\n",
      "  1. 1.\n",
      "    Hernan Fernández, and R. Nelvana. The importance of a clean energy future. Renewable energy research and development: towards a renewable energy future. Renewable energy research and development: toward a clean energy future. Renewable energy research and development: toward a clean energy future. Renewable energy research and development: toward a clean energy future.\n",
      "  2. 2.\n",
      "    Makarová, and H. A. Nieto. Renewable energy research and development: towards a clean energy future. Renewable energy research and development: towards a clean energy future. Renewable energy research and development: towards a clean energy future. Renewable energy research and development: toward a clean energy future.\n",
      "  3. 3.\n",
      "    Nel\n"
     ]
    }
   ],
   "source": [
    "### inference #### before SFT\n",
    "from transformers import OlmoForCausalLM, AutoTokenizer\n",
    "\n",
    "# olmo = OlmoForCausalLM.from_pretrained(\"/home/lvbo/hf_pt/olmo-20m\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/lvbo/hf_pt/olmo-20m\")\n",
    "olmo = OlmoForCausalLM.from_pretrained(\"/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/hf_pt/olmo1b_2024_12_09_01_44_49_6000\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/hf_pt/olmo1b_2024_12_09_01_44_49_6000\", truncate_to=None , eos_token_id=0, pad_token_id=1)\n",
    "# olmo = AutoModelForCausalLM.from_pretrained(\"/home/lvbo/hf_pt/OLMo-1B\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/lvbo/hf_pt/OLMo-1B\")\n",
    "\n",
    "# message = [\"Biology is science, it is based on physics subject. What is your opinion? \"]\n",
    "message = [\"You are an intelligent assistant capable of providing accurate and well-structured answers. Please answer the following question in detail. Question: Explain the significance of renewable energy sources in combating climate change. Provide examples of commonly used renewable energy sources, their advantages, and any challenges associated with their adoption. Make your response structured into three parts: 1) Introduction, 2) Key Examples with Advantages, and 3) Challenges and Conclusion.\"]\n",
    "# message = [\"What is Transformer.\"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "response = olmo.generate(**inputs, max_new_tokens=200, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5f4ae2-67e5-41b3-8544-9d57e6ae214a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 250603 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# olmo = OlmoForCausalLM.from_pretrained(\"/home/lvbo/hf_pt/olmo-20m\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"/home/lvbo/hf_pt/olmo-20m\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m olmo \u001b[38;5;241m=\u001b[39m OlmoForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/sft_pt/olmo1b_2024_12_09_01_44_49\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/sft_pt/olmo1b_2024_12_09_01_44_49\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# , eos_token_id=0, pad_token_id=1)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# olmo = AutoModelForCausalLM.from_pretrained(\"/home/lvbo/hf_pt/OLMo-1B\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"/home/lvbo/hf_pt/OLMo-1B\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# message = [\"Biology is science, it is based on physics subject. What is your opinion? \"]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# message = [\"You are an intelligent assistant capable of providing accurate and well-structured answers. Please answer the following question in detail. Question: Explain the significance of renewable energy sources in combating climate change. Provide examples of commonly used renewable energy sources, their advantages, and any challenges associated with their adoption. Make your response structured into three parts: 1) Introduction, 2) Key Examples with Advantages, and 3) Challenges and Conclusion.\"]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m message \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is physics.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/olmo-lvbo/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:862\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    860\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m         )\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/envs/olmo-lvbo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2089\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2087\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/olmo-lvbo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2311\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2316\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/olmo-lvbo/lib/python3.10/site-packages/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py:106\u001b[0m, in \u001b[0;36mGPTNeoXTokenizerFast.__init__\u001b[0;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, add_bos_token, add_eos_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    105\u001b[0m ):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[0;32m~/miniconda3/envs/olmo-lvbo/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 250603 column 3"
     ]
    }
   ],
   "source": [
    "### inference #### after SFT\n",
    "from transformers import OlmoForCausalLM, AutoTokenizer\n",
    "\n",
    "# olmo = OlmoForCausalLM.from_pretrained(\"/home/lvbo/hf_pt/olmo-20m\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/lvbo/hf_pt/olmo-20m\")\n",
    "olmo = OlmoForCausalLM.from_pretrained(\"/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/sft_pt/olmo1b_2024_12_09_01_44_49\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/a100_nas_lvbo/peixunban/002754_lvbo/SFT/sft_pt/olmo1b_2024_12_09_01_44_49\", truncate_to=None) # , eos_token_id=0, pad_token_id=1)\n",
    "# olmo = AutoModelForCausalLM.from_pretrained(\"/home/lvbo/hf_pt/OLMo-1B\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/lvbo/hf_pt/OLMo-1B\")\n",
    "\n",
    "# message = [\"Biology is science, it is based on physics subject. What is your opinion? \"]\n",
    "# message = [\"You are an intelligent assistant capable of providing accurate and well-structured answers. Please answer the following question in detail. Question: Explain the significance of renewable energy sources in combating climate change. Provide examples of commonly used renewable energy sources, their advantages, and any challenges associated with their adoption. Make your response structured into three parts: 1) Introduction, 2) Key Examples with Advantages, and 3) Challenges and Conclusion.\"]\n",
    "message = [\"What is physics.\"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "response = olmo.generate(**inputs, max_new_tokens=200, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e38816-455c-484e-b7ed-d891f1279d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "llamafactory-cli train lichuan/olmo_1b_sft_s1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e19abd-f2df-48ec-abe2-227c6a42f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "llamafactory-cli webchat lichuan/olmo_1b_sft_predict.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
